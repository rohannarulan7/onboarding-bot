{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "169e17a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import pysolr\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "166dfc46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding vectors to *:* observations:  50%|█████     | 1/2 [00:01<00:01,  1.14s/it]\n"
     ]
    }
   ],
   "source": [
    "def add_vectors():\n",
    "        embedding_host = '35.230.69.135'\n",
    "        embedding_url = f\"http://{embedding_host}:5052/getDiagEmbeddings\"\n",
    "        url = embedding_url\n",
    "        headers = {\n",
    "            'Content-Type': 'application/json'\n",
    "        }\n",
    "        #query = f\"type:OBS AND {SolrConfig.CREATED_DATE}:[{self.extraction_start_time} TO *] AND {SolrConfig.CREATED_BY}:system AND -merged:true AND -active:false\" if self.extraction_start_time else \"type:OBS AND -merged:true AND -active:false\"\n",
    "        query = '*:*'\n",
    "        pred_handle = pysolr.Solr(\"http://localhost:8890/solr/predict\")\n",
    "        all_obs = pred_handle.search(\n",
    "            q=query, \n",
    "            rows=10000\n",
    "        ).docs\n",
    "\n",
    "        to_add = []\n",
    "        progress_desc = f\"Adding vectors to {query} observations\"\n",
    "\n",
    "        for i in tqdm(range(0, len(all_obs), 5), total=(len(all_obs) // 5) + 1, desc=progress_desc):\n",
    "            batch = all_obs[i:i+5]\n",
    "            payload = json.dumps({\n",
    "                \"sentences\": [doc.get('case_description', '') for doc in batch]\n",
    "\n",
    "            })\n",
    "            resp = requests.post(url, headers=headers, data=payload).json()\n",
    "            if \"embeddings\" not in resp:\n",
    "                print(\"Error from embedding API:\", resp)\n",
    "                continue\n",
    "            response = resp[\"embeddings\"]\n",
    "                        \n",
    "\n",
    "            to_add.extend({\"id\": doc[\"id\"], \"vector\": vector} for doc, vector in zip(batch, response))\n",
    "\n",
    "        for i in range(0,len(to_add),5):\n",
    "            pred_handle.add(to_add[i:i+5],fieldUpdates={\"vector\":\"set\"}, commit=True) \n",
    "\n",
    "add_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "64266731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import pysolr\n",
    "\n",
    "def fetch_relevant_docs(user_query: str, top_k: int = 5):\n",
    "    \"\"\"\n",
    "    Takes a user query, generates its embedding, and retrieves top-K relevant documents from Solr.\n",
    "    \"\"\"\n",
    "    # Step 1: Generate embedding for query\n",
    "    embedding_host = '35.230.69.135'\n",
    "    embedding_url = f\"http://{embedding_host}:5052/getDiagEmbeddings\"\n",
    "    url = embedding_url\n",
    "    headers = {\"Content-Type\": \"application/json\"}\n",
    "    payload = json.dumps({\"sentences\": [user_query]})\n",
    "    response = requests.post(url, headers=headers, data=payload).json()\n",
    "    query_embedding = response[\"embeddings\"][0]\n",
    "\n",
    "    # Step 2: Format Solr KNN query\n",
    "    # Solr expects the embedding vector as a JSON array string\n",
    "    vector_str = \"[\" + \",\".join(map(str, query_embedding)) + \"]\"\n",
    "    solr_query = \"{!knn f=vector topK=\" + str(top_k) + \"}\" + vector_str\n",
    "\n",
    "    pred_handle = pysolr.Solr(\"http://localhost:8890/solr/predict\")\n",
    "\n",
    "    # Step 3: Execute search in Solr\n",
    "    results = pred_handle.search(\n",
    "        q=solr_query,\n",
    "        fl=\"id,case_description,score\"  # return id, text, and similarity score\n",
    "    ).docs\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "600b6197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID: OBS-1001, Score: 0.9091029\n",
      "Description: To run the payments service, clone the repo, set the PAYMENT_API_KEY in your environment, and run `docker-compose up`.\n",
      "\n",
      "ID: OBS-1004, Score: 0.8629731\n",
      "Description: The analytics service can be run locally with `python main.py`. Make sure you have installed requirements from `requirements.txt`.\n",
      "\n",
      "ID: OBS-1003, Score: 0.85638607\n",
      "Description: For the notification service, ensure you have an SMTP server configured. Update `config.yaml` with SMTP_HOST and SMTP_PORT.\n",
      "\n",
      "ID: OBS-1002, Score: 0.8495734\n",
      "Description: The authentication service requires a PostgreSQL database. Configure DATABASE_URL in `.env` before starting the service.\n",
      "\n",
      "ID: OBS-1005, Score: 0.8140217\n",
      "Description: When deploying to production, use Kubernetes manifests in the `k8s/` folder. Update secrets before applying configs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I run the payments service?\"\n",
    "docs = fetch_relevant_docs(query, top_k=5)\n",
    "\n",
    "for d in docs:\n",
    "    print(f\"ID: {d['id']}, Score: {d['score']}\")\n",
    "    print(f\"Description: {d['case_description']}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "001f0f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "def generate_solution(user_query: str, relevant_docs: list, model_name: str = \"gpt-4o\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    Takes user query + relevant documents, and generates a solution using the model.\n",
    "    \"\"\"\n",
    "    # Extract just the text content from docs\n",
    "    docs_text = \"\\n\\n\".join(\n",
    "        [doc.get(\"case_description\", \"\") for doc in relevant_docs]\n",
    "    )\n",
    "\n",
    "    # Prompt template\n",
    "    template = \"\"\"You are an onboarding assistant.\n",
    "Use the relevant documents below to answer the user's query.\n",
    "\n",
    "User Query:\n",
    "{query}\n",
    "\n",
    "Relevant Documents:\n",
    "{docs}\n",
    "\n",
    "Answer clearly and step by step, using the documents when possible.\n",
    "\"\"\"\n",
    "\n",
    "    prompt = PromptTemplate.from_template(template)\n",
    "    final_prompt = prompt.format(query=user_query, docs=docs_text)\n",
    "\n",
    "    # Call model\n",
    "    llm = ChatOpenAI(model=model_name, temperature=0)  # deterministic\n",
    "    answer = llm.predict(final_prompt)\n",
    "\n",
    "    return answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5fad43e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jp/vytgqgpd13b2x1l6gnc8_lrc0000gp/T/ipykernel_83969/3730424673.py:32: LangChainDeprecationWarning: The method `BaseChatModel.predict` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  answer = llm.predict(final_prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: To run the payments service, follow these steps:\n",
      "\n",
      "1. **Clone the Repository**: Start by cloning the repository that contains the payments service code to your local machine.\n",
      "\n",
      "2. **Set the Environment Variable**: You need to set the `PAYMENT_API_KEY` in your environment. This is crucial for the service to authenticate and function properly. You can set this in your terminal session or in an environment file, depending on your setup.\n",
      "\n",
      "3. **Run the Service**: Once the repository is cloned and the environment variable is set, navigate to the directory containing the `docker-compose.yml` file. Then, execute the following command to start the payments service:\n",
      "   ```\n",
      "   docker-compose up\n",
      "   ```\n",
      "\n",
      "This will build and run the payments service using Docker, ensuring all necessary dependencies and configurations are in place. docs: [{'id': 'OBS-1001', 'case_description': 'To run the payments service, clone the repo, set the PAYMENT_API_KEY in your environment, and run `docker-compose up`.', 'score': 0.9091029}, {'id': 'OBS-1004', 'case_description': 'The analytics service can be run locally with `python main.py`. Make sure you have installed requirements from `requirements.txt`.', 'score': 0.8629731}, {'id': 'OBS-1003', 'case_description': 'For the notification service, ensure you have an SMTP server configured. Update `config.yaml` with SMTP_HOST and SMTP_PORT.', 'score': 0.85638607}, {'id': 'OBS-1002', 'case_description': 'The authentication service requires a PostgreSQL database. Configure DATABASE_URL in `.env` before starting the service.', 'score': 0.8495734}, {'id': 'OBS-1005', 'case_description': 'When deploying to production, use Kubernetes manifests in the `k8s/` folder. Update secrets before applying configs.', 'score': 0.8140217}]\n"
     ]
    }
   ],
   "source": [
    "query = \"How do I run the payments service?\"\n",
    "docs = fetch_relevant_docs(query, top_k=5)\n",
    "answer = generate_solution(query, docs)\n",
    "\n",
    "print(\"Answer:\", answer, \"docs:\", docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "423114a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n<response>\\n\\n<lst name=\"responseHeader\">\\n  <int name=\"status\">0</int>\\n  <int name=\"QTime\">55</int>\\n</lst>\\n</response>\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pysolr\n",
    "solr = pysolr.Solr(\"http://localhost:8890/solr/predict\")\n",
    "docs = [\n",
    "  {\n",
    "    \"id\": \"OBS-1001\",\n",
    "    \"case_description\": \"To run the payments service, clone the repo, set the PAYMENT_API_KEY in your environment, and run `docker-compose up`.\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"OBS-1002\",\n",
    "    \"case_description\": \"The authentication service requires a PostgreSQL database. Configure DATABASE_URL in `.env` before starting the service.\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"OBS-1003\",\n",
    "    \"case_description\": \"For the notification service, ensure you have an SMTP server configured. Update `config.yaml` with SMTP_HOST and SMTP_PORT.\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"OBS-1004\",\n",
    "    \"case_description\": \"The analytics service can be run locally with `python main.py`. Make sure you have installed requirements from `requirements.txt`.\"\n",
    "  },\n",
    "  {\n",
    "    \"id\": \"OBS-1005\",\n",
    "    \"case_description\": \"When deploying to production, use Kubernetes manifests in the `k8s/` folder. Update secrets before applying configs.\"\n",
    "  }\n",
    "]\n",
    "solr.add(docs)\n",
    "solr.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
